import os
from dotenv import load_dotenv
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate
from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint

from galapassistant.apps.assistant.services.retriever_service import RetrieverTool

load_dotenv()

LLM_MODEL_NAME = "HuggingFaceH4/zephyr-7b-beta"  # Other models available
FILE_PATH = os.path.join(
    os.path.dirname(__file__),
    "..",
    ".knowledge_base",
    "the Origin of Species.txt"
)

class AssistantLLMService:
    """
    Service class for generating responses using an LLM.
    """
    def __init__(self):
        """
        Initializes the AssistantLLMService by creating an instance of the LLM.
        """
        self.llm = HuggingFaceEndpoint(
            repo_id=LLM_MODEL_NAME,
            huggingfacehub_api_token=os.getenv("HUGGINGFACEHUB_API_TOKEN"),
            temperature=0.3,
        )
        chat_model = ChatHuggingFace(llm=self.llm)

        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "Your name is GalapAssistant, the smart assistant from GalÃ¡pagos Islands. You are a specialized and polite assistant that knows everything about the book The Origin of Species, because you have this book as your knowledge base. Respond only to the question asked. Response should be concise, comprehensive and relevant to the question. If the answer cannot be deduced from the context, do not give an answer and report this. Here is the context, your knowledge base."
                ),
                (
                    "human",
                    "Here is the question you need to answer using the information contained in the context."
                ),
                (
                    "assistant",
                    "{agent_scratchpad}"
                )
            ]
        )
        retriever = RetrieverTool()
        retriever_tool = [retriever]
        self.agent = create_tool_calling_agent(
            tools=retriever_tool,
            llm=chat_model,
            prompt=prompt
        )

    def generate_answer(self, user_query: str) -> str:
        """
        Generate a response from the LLM based on the provided query.
        
        Args:
            user_query (str): The user's query.
        
        Returns:
            str: The response generated by the LLM.
        """
        response = self.agent.run(user_query)

        return response
