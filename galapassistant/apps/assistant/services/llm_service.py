import os
from dotenv import load_dotenv
from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint
from smolagents import HfApiModel, ToolCallingAgent
from galapassistant.apps.assistant.services.embedding_service import EmbeddingService
from galapassistant.apps.assistant.services.rag_service import RetrieverTool


load_dotenv()

LLM_MODEL_NAME = "HuggingFaceH4/zephyr-7b-beta"
FILE_PATH = os.path.join(
    os.path.dirname(__file__),
    "..",
    ".knowledge_base",
    "the Origin of Species.txt"
)

class AssistantLLMService:
    """
    Service class for generating responses using an LLM.
    """
    def __init__(self):
        """
        Initializes the AssistantLLMService by creating an instance of the LLM.
        """
        self.model = HfApiModel(
            model_id=LLM_MODEL_NAME, ### "meta-llama/Llama-3.1-70B-Instruct",
            token=os.getenv("HUGGINGFACEHUB_API_TOKEN")
        )

        # self.llm = HuggingFaceEndpoint(
        #     repo_id="HuggingFaceH4/zephyr-7b-beta",
        #     huggingfacehub_api_token=os.getenv("HUGGINGFACEHUB_API_TOKEN"),
        #     temperature=0.7,
        #     max_new_tokens=256,
        # )

    def get_response(self, user_query: str) -> str:
        """
        Generate a response from the LLM based on the provided query.

        Args:
            query (str): The user's query.

        Returns:
            str: The response generated by the LLM.
        """
        retriever_tool = RetrieverTool()
        agent = ToolCallingAgent(tools=[retriever_tool], model=self.llm)
        response = agent.run(user_query)

        return response
