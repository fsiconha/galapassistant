import os
from dotenv import load_dotenv
from langchain import LLMChain
from langchain.agents import (Tool, AgentExecutor, ZeroShotAgent)
from langchain_core.output_parsers import StrOutputParser
from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace
from langchain.prompts import PromptTemplate


from galapassistant.apps.assistant.services.retriever_service import RetrieverTool
from galapassistant.apps.assistant.services.utils import AgentUtils


load_dotenv()

LLM_MODEL_NAME = "HuggingFaceH4/zephyr-7b-beta" ##Other models: "MaziyarPanahi/calme-3.1-instruct-78b", "meta-llama/Llama-3.1-70B-Instruct"
FILE_PATH = os.path.join(
    os.path.dirname(__file__),
    "..",
    ".knowledge_base",
    "the Origin of Species.txt"
)

class AssistantLLMService:
    """
    Service class for generating responses using an LLM.
    """
    def __init__(self):
        """
        Initializes the AssistantLLMService by creating an instance of the LLM.
        """
        PREFIX = """You are an AI assistant that follows the ReAct pattern.
        You have access to a single tool: 'OriginOfSpeciesRetriever'.

        You should decide carefully whether:
        1. The user's question is about Darwinism and might be answered by the book 'The Origin of Species'.
        - If so, use the tool.
        2. Otherwise, answer directly from your own knowledge without using the tool.

        When you do use the tool, follow this format exactly:
        - Thought: "I need to look something up about The Origin of Species."
        - Action: "OriginOfSpeciesRetriever" with the question
        - Observation: [result from the book]
        - Thought: "Now I have what I need."
        - Final Answer: [your final answer for the user]

        -----

        """
        FORMAT_INSTRUCTIONS = """You are free to either:
        - Answer directly (if no book lookup is needed), or
        - Use the tool for a relevant Darwin-based question.

        Remember to follow the ReAct pattern strictly when using the tool!
        """

        SUFFIX = """
        Begin!

        User's question: {input}
        {agent_scratchpad}
        """

        utils = AgentUtils()
        self.retriever_tool = utils.get_retriever()

        self.prompt_template = ZeroShotAgent.create_prompt(
            tools=[retriever_tool],
            prefix=PREFIX,
            suffix=SUFFIX,
            format_instructions=FORMAT_INSTRUCTIONS,
            input_variables=["input", "agent_scratchpad"],
        )

    def build_llm(self):
        llm = HuggingFaceEndpoint(
            repo_id=LLM_MODEL_NAME,
            task="text-generation",
            max_new_tokens=512,
            do_sample=False,
            repetition_penalty=1.03,
        )
        chat_model = ChatHuggingFace(llm=llm, verbose=True)

        return chat_model

    def build_agent(self):
        chat_model = self.build_llm()
        llm_chain = LLMChain(llm=chat_model, prompt=self.prompt_template)

        agent = ZeroShotAgent(
            llm_chain=llm_chain,
            tools=[self.retriever_tool],
            verbose=True
        )

        agent_executor = AgentExecutor.from_agent_and_tools(
            agent=agent,
            tools=[self.retriever_tool],
            verbose=True
        )

        return agent_executor

    def generate_answer(self, user_query: str) -> str:
        """
        Generate a response from the LLM based on the provided query.

        Args:
            query (str): The user's query.

        Returns:
            str: The response generated by the LLM.
        """
        agent_executor = self.build_agent()
        response = agent_executor.run(user_query)

        return response
