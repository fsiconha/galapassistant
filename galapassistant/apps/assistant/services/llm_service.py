import os
from dotenv import load_dotenv
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_huggingface import HuggingFaceEndpoint
from langchain.prompts import ChatPromptTemplate

from galapassistant.apps.assistant.services.retriever_service import EmbeddingService
# from galapassistant.apps.assistant.services.retriever_service import RetrieverService


load_dotenv()

LLM_MODEL_NAME = "HuggingFaceH4/zephyr-7b-beta" ##Other models: "MaziyarPanahi/calme-3.1-instruct-78b", "meta-llama/Llama-3.1-70B-Instruct"
FILE_PATH = os.path.join(
    os.path.dirname(__file__),
    "..",
    ".knowledge_base",
    "the Origin of Species.txt"
)

class AssistantLLMService:
    """
    Service class for generating responses using an LLM.
    """
    def __init__(self):
        """
        Initializes the AssistantLLMService by creating an instance of the LLM.
        """
        llm = HuggingFaceEndpoint(
            repo_id=LLM_MODEL_NAME,
            huggingfacehub_api_token=os.getenv("HUGGINGFACEHUB_API_TOKEN"),
            temperature=0.7,
            max_new_tokens=256,
        )
        prompt_template = """
            Using the information contained in the context, give a comprehensive answer to the question.
            Respond only to the question asked, response should be concise and relevant to the question.
            If the answer cannot be deduced from the context, do not give an answer.
            The context is a book called The Origin of Species,
            and it introduces the theory of evolution by natural selection,
            explaining how species adapt and change over generations.: {context}
            Question: {question}
        """
        prompt = ChatPromptTemplate.from_template(prompt_template)
        vector_store = EmbeddingService().build_vector_database()
        retriever = vector_store.as_retriever()
        self.rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

    def get_response(self, user_query: str) -> str:
        """
        Generate a response from the LLM based on the provided query.

        Args:
            query (str): The user's query.

        Returns:
            str: The response generated by the LLM.
        """
        response = self.rag_chain.invoke(user_query)

        return response
