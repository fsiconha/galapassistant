import os
from dotenv import load_dotenv
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_huggingface import HuggingFaceEndpoint
from langchain.prompts import ChatPromptTemplate

from galapassistant.apps.assistant.services.retriever_service import RetrieverService


load_dotenv()

LLM_MODEL_NAME = "HuggingFaceH4/zephyr-7b-beta" ##Other models: "MaziyarPanahi/calme-3.1-instruct-78b", "meta-llama/Llama-3.1-70B-Instruct"
FILE_PATH = os.path.join(
    os.path.dirname(__file__),
    "..",
    ".knowledge_base",
    "the Origin of Species.txt"
)

class AssistantLLMService:
    """
    Service class for generating responses using an LLM.
    """
    def __init__(self):
        """
        Initializes the AssistantLLMService by creating an instance of the LLM.
        """
        self.llm = HuggingFaceEndpoint(
            repo_id=LLM_MODEL_NAME,
            huggingfacehub_api_token=os.getenv("HUGGINGFACEHUB_API_TOKEN"),
            temperature=0.3,
            max_new_tokens=256,
        )
        prompt_instructions = [
            {
                "role": "system",
                "content": """ You are a specialized and polite assistant that knows everything about the book The Origin of Species,
                because you have this book as your knowledge base.
                Respond only to the question asked. Response should be concise, comprehensive and relevant to the question.
                If the answer cannot be deduced from the context, do not give an answer and report this.
                Here is the context, your knoledge base.
                Context: {context}""",
            },
            {
                "role": "user",
                "content": """ Here is the question you need to answer using the information contained in the context.
                Question: {question}""",
            },
        ]

        self.prompt = ChatPromptTemplate(prompt_instructions)

    def get_response(self, user_query: str) -> str:
        """
        Generate a response from the LLM based on the provided query.

        Args:
            query (str): The user's query.

        Returns:
            str: The response generated by the LLM.
        """
        retriever = RetrieverService().retrieve(user_query)
        rag_chain = self.prompt | self.llm | StrOutputParser()
        chain_input = {"context": retriever, "question": user_query}
        response = rag_chain.invoke(chain_input)

        return response
