import os
from dotenv import load_dotenv
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_huggingface import HuggingFaceEndpoint
from langchain.prompts import ChatPromptTemplate

from galapassistant.apps.assistant.services.retriever_service import RetrieverService

import logging
import gc
from http.client import RemoteDisconnected
from requests.exceptions import ConnectionError, Timeout, RequestException

load_dotenv()

LLM_MODEL_NAME = "HuggingFaceH4/zephyr-7b-beta"

class AssistantLLMService:
    """
    Service class for generating responses using an LLM.
    """
    def __init__(self):
        """
        Initializes the AssistantLLMService by creating an instance of the LLM.
        """
        prompt_instructions = [
            {
                "role": "system",
                "content": """ You are a specialized and polite assistant that knows everything about the book The Origin of Species,
                because you have this book as your knowledge base.
                Respond only to the question asked. Response should be concise, comprehensive and relevant to the question.
                If the answer cannot be deduced from the context, do not give an answer and report this.
                Here is the context, your knoledge base.
                Context: {context}""",
            },
            {
                "role": "user",
                "content": """ Here is the question you need to answer using the information contained in the context.
                Question: {question}""",
            },
        ]

        self.prompt = ChatPromptTemplate(prompt_instructions)

    def _create_llm(self):
        self.llm = HuggingFaceEndpoint(
            repo_id=LLM_MODEL_NAME,
            huggingfacehub_api_token=os.getenv("HUGGINGFACEHUB_API_TOKEN"),
            temperature=0.3,
            max_new_tokens=256,
        )

    def get_response(self, user_query: str) -> str:
        """
        Generate a response from the LLM based on the provided query.

        Args:
            query (str): The user's query.

        Returns:
            str: The response generated by the LLM.
        """
        try:
            retriever = RetrieverService().retrieve(user_query)
            rag_chain = self.prompt | self.llm | StrOutputParser()
            chain_input = {"context": retriever, "question": user_query}
            response = rag_chain.invoke(chain_input)
            return response

        except (RemoteDisconnected, ConnectionError, Timeout) as conn_err:
            logging.warning(f"Connection issue detected: {conn_err}. Reinitializing client and retrying...")

            del self.llm
            gc.collect()
            self._create_llm()

            try:
                retriever = RetrieverService().retrieve(user_query)
                rag_chain = self.prompt | self.llm | StrOutputParser()
                chain_input = {"context": retriever, "question": user_query}
                response = rag_chain.invoke(chain_input)
                return response
            except Exception as retry_err:
                logging.error(f"Retry failed: {retry_err}")
                return "Sorry, the assistant is temporarily unavailable. Please try again later."

        except RequestException as req_err:
            logging.error(f"RequestException: {req_err}")
            return "A network error occurred while contacting the assistant service."

        except Exception as e:
            logging.exception(f"Unexpected error: {e}")
            return "An unexpected error occurred while processing your request."