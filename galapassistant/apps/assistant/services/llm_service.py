import os
from dotenv import load_dotenv
from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
)
import torch
from galapassistant.apps.assistant.services.embedding_service import EmbeddingService


load_dotenv()

LLM_MODEL_NAME = "HuggingFaceH4/zephyr-7b-beta"

class AssistantLLMService:
    """
    Service class for generating responses using an LLM.
    """

    def __init__(self):
        """
        Initializes the AssistantLLMService by creating an instance of the LLM.
        """
        # self.llm = HuggingFaceEndpoint(
        #     repo_id="HuggingFaceH4/zephyr-7b-beta",
        #     huggingfacehub_api_token=os.getenv("HUGGINGFACEHUB_API_TOKEN"),
        #     temperature=0.7,
        #     max_new_tokens=256,
        # )

        # self.bnb_config = BitsAndBytesConfig(
        #     load_in_4bit=True,
        #     bnb_4bit_use_double_quant=True,
        #     bnb_4bit_quant_type="nf4",
        #     bnb_4bit_compute_dtype=torch.bfloat16,
        # )
        self.model = AutoModelForCausalLM.from_pretrained(
            LLM_MODEL_NAME,
            # quantization_config=self.bnb_config
        )
        self.tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)

        self.llm = pipeline(
            model=self.model,
            tokenizer=self.tokenizer,
            task="text-generation",
            do_sample=True,
            temperature=0.2,
            repetition_penalty=1.1,
            return_full_text=False,
            max_new_tokens=500,
        )

        prompt_in_chat_format = [
            {
                "role": "system",
                "content": """
                Using the information contained in the context, give a comprehensive answer to the question.
                Respond only to the question asked, response should be concise and relevant to the question.
                Provide the number of the source document when relevant.
                If the answer cannot be deduced from the context, do not give an answer.
                """,
            },
            {
                "role": "user",
                "content": """
                Context: {context}
                ---
                Now here is the question you need to answer.

                Question: {question}""",
            },
        ]
        self.rag_prompt_template = self.tokenizer.apply_chat_template(
            prompt_in_chat_format, tokenize=False, add_generation_prompt=True
        )

    def data_retriever(self, user_query):
        vector_store = EmbeddingService.build_vector_database()
        retrieved_docs = vector_store.similarity_search(
            query=user_query, k=5
        )

        return retrieved_docs

    def get_response(self, user_query: str) -> str:
        """
        Generate a response from the LLM based on the provided query.

        Args:
            query (str): The user's query.

        Returns:
            str: The response generated by the LLM.
        """
        retrieved_docs = self.data_retriever(user_query=user_query)
        retrieved_docs_text = [doc.page_content for doc in retrieved_docs]
        context = "\nExtracted documents:\n"
        context += "".join(
            [
                f"Document {str(i)}:::\n" + doc for i, doc in enumerate(retrieved_docs_text)
            ]
        )
        final_prompt = self.rag_prompt_template.format(
            question="How to create a pipeline object?", context=context
        )

        return self.llm(final_prompt)[0][user_query]
