import os
from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import ChatPromptTemplate
from smolagents import HfApiModel, ToolCallingAgent

from galapassistant.apps.assistant.services.retriever_service import RetrieverTool


load_dotenv()

LLM_MODEL_NAME = "HuggingFaceH4/zephyr-7b-beta" ##Other models: "MaziyarPanahi/calme-3.1-instruct-78b", "meta-llama/Llama-3.1-70B-Instruct"
FILE_PATH = os.path.join(
    os.path.dirname(__file__),
    "..",
    ".knowledge_base",
    "the Origin of Species.txt"
)

class AssistantLLMService:
    """
    Service class for generating responses using an LLM.
    """
    def __init__(self):
        """
        Initializes the AssistantLLMService by creating an instance of the LLM.
        """
        self.llm = HfApiModel(
            model_id=LLM_MODEL_NAME,
            token=os.getenv("HUGGINGFACEHUB_API_TOKEN"),
            temperature=0.3,
        )
        prompt_instructions = [
            {
                "role": "system",
                "content": "Your name is GalapAssistant, the smart assistant from GalÃ¡pagos Islands. You are a specialized and polite assistant that knows everything about the book The Origin of Species, because you have this book as your knowledge base. Respond only to the question asked. Response should be concise, comprehensive and relevant to the question. If the answer cannot be deduced from the context, do not give an answer and report this. Here is the context, your knoledge base. Context: {context}",
            },
            {
                "role": "user",
                "content": "Here is the question you need to answer using the information contained in the context. Question: {question}",
            },
        ]
        self.prompt = ChatPromptTemplate(prompt_instructions)

    def generate_answer(self, user_query: str) -> str:
        """
        Generate a response from the LLM based on the provided query.

        Args:
            query (str): The user's query.

        Returns:
            str: The response generated by the LLM.
        """
        retriever = RetrieverTool()
        self.agent = ToolCallingAgent(
            tools=[retriever], model=self.llm, verbosity_level=2
        )
        agent_chain = self.prompt | self.agent | StrOutputParser()
        chain_input = {"context": retriever, "question": user_query}
        response = agent_chain.invoke(chain_input)

        return response
